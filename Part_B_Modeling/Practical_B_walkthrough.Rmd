---
title: "Practical B"
author: "Jolien Cremers (adapted from Gerko Vink)"
date: "Statistical Programming in R"
output: html_document
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
require(MASS)
require(DAAG)
require(ggplot2)
require(splines)
```


---

### Exercises

---

The following packages are required for this practical:
```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(magrittr)
library(mice)
library(ggplot2)
library(DAAG)
library(MASS)
library(broom)
library(tidyr)
```
These exercises deal with modeling data in `R`. We will explore the workhorse `lm` function for linear modeling as well as a robust regression function found in the `MASS` package and the `glm` function for generalized linear models (including logistic regression). You will see that the interface to modeling is very similar across packages. Model output can be less similar, something which the `broom` package attempts to remedy. 

---

#### Using the `lm` function to model the stretch of rubber bands. 

The data sets `elastic1` and `elastic2` from the package `DAAG` were obtained
using the same apparatus, including the same rubber band, as the data frame
`elasticband`. 

As this is a very small data set, some of the issues met in modeling it are such as we will not often run into when working with official statistics. 

1. **Using a different symbol and/or a different color, plot the data
from the two data frames `elastic1` and `elastic2` on the same graph. Do the two
sets of results appear consistent?**

```{r}
elastic <- bind_rows("elastic1"=elastic1, "elastic2"=elastic2, .id = "source" )

elastic %>%
  ggplot(aes(stretch, distance, colour = source)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  scale_fill_manual(values = c("blue", "green"))

```

The results seem very consistent: Data set `elastic2` has more observations over a larger range, but both sets result in roughly the same regression line. Data set `elastic1` seems to have an *odd-one-out* value.

---

2. **For each of the data sets `elastic1` and `elastic2`, determine the regression of
distance on stretch. In each case determine:**

- fitted values and standard errors of fitted values and
- the $R^2$ statistic.

Compare the two sets of results. What is the key difference between the two sets
of data?

First we run the two models:
```{r}
fit1 <- lm(distance ~ stretch, data=elastic1)

fit2 <- lm(distance ~ stretch, data=elastic2)
```

and then we compare the fitted values
```{r}
fit1 %>% predict(se.fit = TRUE)
fit2 %>% predict(se.fit = TRUE)
```
We see that `fit1` (based on `elastic1`) has a larger residual standard deviation (i.e. `$residual.scale`). 

To get the $R^2$ we can run a summary on the fitted models:
```{r}
fit1 %>% summary()
fit2 %>% summary()
```

Or we can grab the $R^2$ directly from the object without a pipe - note that $R^2$ lives in the `summary` of the fit object, not in the fit object itself.
```{r}
summary(fit1)$r.squared
summary(fit2)$r.squared
```
The model based on `elastic2` has smaller standard errors and a much larger $R^2$.
This is due to the larger range of values in `elastic2`, and the absence of an outlier.

---

3. **Look at the plot function for lm class. (Hint: ?plot.lm). The function produces a number of  diagnostic plots for fitted linear models. Study the *residual vs leverage* plots for both models.**

```{r}
?plot.lm
```
Note that you can ask for specific plots using the `which` argument to the function. The plots produced are "Residuals vs Fitted", "Normal Q-Q", "Scale-Location", "Cook's distance","Residuals vs Leverage" and "Cook's dist vs Leverage" - by default, plots 1, 2, 3 and 5 are shown. 

```{r}
fit1 %>% plot(which = 5) #the fifth plot is the residuals vs leverage plot
fit2 %>% plot(which = 5)
```

For `elastic1`, case 2 has the largest influence on the estimation. However, it is not the case with the largest residual:
```{r}
fit1$residuals
```

As we can see, case 7 has the largest residual.

---

Because there is a single value that influences the estimation and is somewhat different than the other values, a robust form of regression may be advisable to obtain more stable estimates. 

---

#### Continuing modeling the stretch of rubber bands using robust regression. 

4. **Use the robust regression function `rlm()` from the `MASS` package to fit lines to the data in `elastic1` and `elastic2`. Compare the results with those from use of `lm()`:**

- residuals
- regression coefficients, 
- standard errors of coefficients, 
- plots of residuals against fitted values.

First, we run the same models again with `rlm()`
```{r}
fit1.rlm <- rlm(distance ~ stretch, data = elastic1)

fit2.rlm <- rlm(distance ~ stretch, data = elastic2)
```

and then we look at the coefficients and the residuals - the `broom` package is useful for tidying up the output of regression models
```{r}
bind_rows( "lm" = tidy(fit1), "rlm"=tidy(fit1.rlm), .id = "model")

bind_rows( "lm" = tidy(fit2), "rlm"=tidy(fit2.rlm), .id = "model")
```

We see that the coefficients for `elastic1` are different for `lm()` and `rlm()`. The coefficients for `elastic2` are very similar. 

The standard errors for the estimates for `elastic1` have become much smaller with `rlm()` compared to standard `lm()` estimation. The standard errors for `elastic2` are very similar. 

To study the residuals:
```{r}
tibble("lm" = residuals(fit1),"rlm" = residuals(fit1.rlm))

tibble("lm" = residuals(fit2),  "rlm" = residuals(fit2.rlm))
```

The residual trend for both models is very similar. Remember that different values will still be different under robust analyses; they are only given less influence. 

---

To plot the residuals against the fitted values:
```{r}
plot(fit1, which = 1, add.smooth = "FALSE", col = "blue", main = "elastic1")
points(residuals(fit1.rlm) ~ fitted(fit1.rlm), col = "orange")

plot(fit2, which = 1, add.smooth = "FALSE", col = "blue", main = "elastic2")
points(residuals(fit2.rlm) ~ fitted(fit2.rlm), col = "orange")
```

The case 2 residual in elastic1 is smaller in the robust regression. This is
because the case had less weight in the `rlm()` estimation of the coefficients than
in the ordinary `lm()` regression.

---

#### Rubber bands continued - predicting outcomes on new data from a fitted model

5. **Use the `elastic2` variable `stretch` to obtain predictions on the model fitted on `elastic1`.**

```{r}
pred <- augment(fit1, newdata = elastic2)
pred
```

---

6. **Now make a scatterplot to investigate similarity between plot the predicted values against the observed values for `elastic2`**
```{r}
pred %>%
  rename( observed = distance, fitted = .fitted) %>% 
  pivot_longer(cols = c( observed, fitted), 
               names_to = "source",
               values_to = "distance") %>% 
  ggplot(aes(stretch, distance, colour = source)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

The predicted values are very similar to the observed values:
```{r}
pred %>% 
  ggplot(aes(distance, .fitted)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1)
  
```

They do not strictly follow the straight line because there is some modeling error: we use `elastic1`'s model to predict `elastic2`'s distance [error source 1] and we compare those predictions to `elastic2`'s observed distance [error source 2]. However, if you consider the modeling, these predictions are very accurate and have high correlations with the observed values:
```{r}
bind_cols(distance = elastic2$distance, predicted = pred) %>%
  cor() 
```

---

#### Logistic regression using the glm function

Often, a linear normal regression is not what we want to use. A typical example is when we have data with two classes and want to do logistic regression. Googling (or background knowledge and typing `?glm` into the console) suggests that the `glm` function is the obvious choice here. 

In the next exercises, we explore the anesthetic dataset: Thirty patients were given an anesthetic agent maintained at a predetermined level (conc) for 15 minutes before making an incision. It was then noted whether the patient moved, i.e. jerked or twisted

---

7. **Look at the documentation for the `anesthetic` dataset. Regresss the binary outcome nomove on the anesthetic concentration**
```{r}
help("anesthetic")
anesthetic %>% head()
```
The data contains the variables move (movement indicator), conc (anesthetic concentration), logconc (logarithm of concentration) and nomove (the complement of move)

```{r}
glm(nomove ~ conc, family = binomial(link="logit"), data=anesthetic) %>% summary()
```

---

8. **Logistic regression can also be performed on aggregated datasets, regressing the proportion with the outcome on the predictor variable(s). Construct the aggregated data frame listing number of patients moving and not moving for each level of anesthetic, and perform the previous regression on the aggregated data**

One approach to aggregating the data would be the following - see also the `aggregate` function. 
```{r}
anestot <- anesthetic %>% 
  group_by( conc ) %>% 
  summarise( move = sum(move), nomove = sum(nomove)) %>% 
  mutate( total = move + nomove
        , prop = nomove / total)

anestot
```

To run logistic regression on the aggregated table, we model prop as a function of conc, and use the weights argument:
```{r}
glm(prop ~ conc, 
    family = binomial(link="logit"), 
    weights = total, 
    data=anestot) %>% 
  summary()
```

---

9. **We are assuming a linear relationship between move and conc. Fit a model including a quadratic term, and one using a spline to model the relationship**

One approach is to include the square of conc as a variable in the data, another is to use the `I` function to isolate the square term in the formula.

```{r}
glm(nomove ~ conc + I(conc^2), family = binomial(link="logit"), data=anesthetic) %>% summary()
```

Base `R` includes functions for fitting simple polynomial and smoothing splines. More functions are included in the `splines` package. We shall use a natural spline with three degrees of freedom. 

```{r}
glm(nomove ~ ns(conc, df = 3), family = binomial(link="logit"), data=anesthetic) %>% summary()
```

---

End of `Practical`. 
